import streamlit as st
import librosa
import numpy as np
import speech_recognition as sr
import pykakasi
import tempfile
import openai
import requests
from pydub import AudioSegment
from pydub.silence import detect_nonsilent
from traits_and_prompts import Extraversion, Openness, Conscientiousness, Agreeableness, E_category, O_category, C_category, A_category, instruction_1, example_1, instruction_2


# OpenAI APIã‚­ãƒ¼ã®è¨­å®š
openai.api_key = st.secrets["OPENAI_API_KEY"]

# åŸºæº–å€¤ã®è¨­å®š
pitch_threshold = [154, 175, 214]  # ãƒ”ãƒƒãƒã®é–¾å€¤
voice_speed_threshold = [5.7, 6.6, 7.1]  # ç™ºè©±é€Ÿåº¦ã®é–¾å€¤
contrast_threshold = 23.5  # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯¾æ¯”ã®é–¾å€¤
silence_thresh = -40  # ç„¡éŸ³ã®é–¾å€¤


def compute_features(audio_path):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸»è¦ãªãƒ”ãƒƒãƒã¨å¹³å‡ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯¾æ¯”ã‚’è¨ˆç®—ã™ã‚‹

    Args:
        audio_path (str): éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        tuple: ä¸»è¦ãªãƒ”ãƒƒãƒã€å¹³å‡ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯¾æ¯”
    """
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    y, sr = librosa.load(audio_path, sr=None)

    # YINã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ãƒ”ãƒƒãƒã‚’æ¤œå‡º
    pitches = librosa.yin(y, fmin=80, fmax=400)

    # NaNå€¤ã‚„ç„¡é™å¤§ã®å€¤ã‚’é™¤å»ã™ã‚‹ãŸã‚ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    valid_pitches = pitches[~np.isnan(pitches)]

    # ä¸»è¦ãªãƒ”ãƒƒãƒã‚’è¨ˆç®—
    main_pitch = np.mean(valid_pitches) if len(valid_pitches) > 0 else 0

    # å¹³å‡ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯¾æ¯”ã‚’è¨ˆç®—
    spectral_contrasts = librosa.feature.spectral_contrast(y=y, sr=sr)
    avg_spectral_contrast = np.mean(spectral_contrasts)

    return main_pitch, avg_spectral_contrast


def categorize_audio_by_average(audio_path):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å¹³å‡ãƒ”ãƒƒãƒã¨ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯¾æ¯”ã‚’ã‚‚ã¨ã«ã‚«ãƒ†ã‚´ãƒªåŒ–ã—ã€ç”»åƒç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹è‰²ã®æŒ‡é‡ã‚’è¿”ã™

    Args:
        audio_path (str): éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        tuple: ãƒ”ãƒƒãƒã®ã‚«ãƒ†ã‚´ãƒªã€ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯¾æ¯”ã®ã‚«ãƒ†ã‚´ãƒªã€ãƒ”ãƒƒãƒå€¤ã€ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯¾æ¯”å€¤ã€ãƒ”ãƒƒãƒã®é«˜ä½ã€ç”»åƒç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹è‰²ã®æŒ‡é‡
    """
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å¾´ã‚’è¨ˆç®—
    pitch, spectral_contrast = compute_features(audio_path)

    # ãƒ”ãƒƒãƒã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–
    if pitch <= pitch_threshold[0]:
        pitch_category = 0
        pitch_height = "low pitch"
    elif pitch <= pitch_threshold[1]:
        pitch_category = 1
        pitch_height = "low pitch"
    elif pitch <= pitch_threshold[2]:
        pitch_category = 2
        pitch_height = "high pitch"
    else:
        pitch_category = 3
        pitch_height = "high pitch"

    # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯¾æ¯”ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–
    if spectral_contrast <= contrast_threshold:
        contrast_category = "husky"
    else:
        contrast_category = "clear"

    # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯¾æ¯”ã¨ãƒ”ãƒƒãƒã®é«˜ä½ã‹ã‚‰ç”»åƒç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹è‰²ã®æŒ‡é‡ã‚’æ±ºå®š
    if contrast_category == "husky" and pitch_height == "low pitch":
        image_color_lineage = "Choose one image color from [navy, cobalt blue, Lavender, Violet ]"
    elif contrast_category == "husky" and pitch_height == "high pitch":
        image_color_lineage = "Choose one image color from [rose pink, coral pink, yellow, gold]"
    elif contrast_category == "clear" and pitch_height == "low pitch":
        image_color_lineage = "Choose one image color from [Red, Green, emerald green, , light brown]"
    else:  # contrast_category == "clear" and pitch_height == "high pitch"
        image_color_lineage = "Choose one image color from [Light Blue, turquoise blue, white, silver]"

    return pitch_category, contrast_category, pitch, spectral_contrast, pitch_height, image_color_lineage

def speaking_rate_by_audio(audio_path):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç™ºè©±é€Ÿåº¦ã‚’è¨ˆç®—ã™ã‚‹

    Args:
        audio_path (str): éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        tuple: 1ç§’ã‚ãŸã‚Šã®æ–‡å­—æ•°ã€ç™ºè©±é€Ÿåº¦ã®ã‚«ãƒ†ã‚´ãƒª
    """
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    sound = AudioSegment.from_file(audio_path)
    nonsilent_ranges = []
    thresh = silence_thresh

    # ç„¡éŸ³ã§ãªã„åŒºé–“ã‚’æ¤œå‡ºã™ã‚‹
    while not nonsilent_ranges:
        nonsilent_ranges = detect_nonsilent(sound, min_silence_len=100, silence_thresh=thresh)
        thresh -= 10

    # é–‹å§‹æ™‚åˆ»ã¨çµ‚äº†æ™‚åˆ»ã‚’å–å¾—ã™ã‚‹
    start_time = nonsilent_ranges[0][0]
    end_time = nonsilent_ranges[-1][1]

    # éŸ³å£°ã®å§‹ã¾ã‚Šã¨çµ‚ã‚ã‚Šã‚’åˆ‡ã‚Šå–ã‚‹
    processed_sound = sound[start_time:end_time]

    # éŸ³å£°èªè­˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    recognizer = sr.Recognizer()

    # ä¸€æ™‚çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as temp_file:
        temp_file_path = temp_file.name
        processed_sound.export(temp_file_path, format="wav")

        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        with sr.AudioFile(temp_file_path) as source:
            audio_data = recognizer.record(source)

    # Google Web Speech APIã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    try:
        text = recognizer.recognize_google(audio_data, language='ja-JP')
    except (sr.RequestError, sr.UnknownValueError) as e:
        raise ValueError(f"Speech recognition failed: {e}")

    # pykakasiã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    kakasi = pykakasi.kakasi()

    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã²ã‚‰ãŒãªã«å¤‰æ›
    conversion_results = kakasi.convert(text)
    hiragana_text = ''.join([item['hira'] for item in conversion_results])

    # ã²ã‚‰ãŒãªã®æ–‡å­—æ•°ã‚’å–å¾—
    hiragana_length = len(hiragana_text)

    # æ™‚é–“ã®å·®ã‚’ç§’å˜ä½ã§è¨ˆç®—
    time_difference = (end_time - start_time) / 1000.0  # pydubã¯ãƒŸãƒªç§’å˜ä½ãªã®ã§1000ã§å‰²ã‚‹

    # 1ç§’ã‚ãŸã‚Šã®æ–‡å­—æ•°ã‚’è¨ˆç®—
    characters_per_second = hiragana_length / time_difference

    # 1ç§’ã‚ãŸã‚Šã®æ–‡å­—æ•°ã«åŸºã¥ã„ã¦ç™ºè©±é€Ÿåº¦ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–
    if characters_per_second < voice_speed_threshold[0]:
       speaking_rate = 0
    elif voice_speed_threshold[0] <= characters_per_second < voice_speed_threshold[1]:
        speaking_rate = 1
    elif voice_speed_threshold[1] <= characters_per_second < voice_speed_threshold[2]:
        speaking_rate = 2
    else:
        speaking_rate = 3

    return characters_per_second, speaking_rate

def generate_first_prompt(pitch_category, contrast_category, speaking_rate, pitch_height, image_color_lineage):
    """
    å„æ€§æ ¼ç‰¹æ€§ã«å¯¾å¿œã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æœ€åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹
    
    Args:
        pitch_category (int): ãƒ”ãƒƒãƒã®ã‚«ãƒ†ã‚´ãƒª
        contrast_category (str): ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯¾æ¯”ã®ã‚«ãƒ†ã‚´ãƒª
        speaking_rate (int): ç™ºè©±é€Ÿåº¦ã®ã‚«ãƒ†ã‚´ãƒª
        pitch_height (str): ãƒ”ãƒƒãƒã®é«˜ä½
        image_color_lineage (str): ç”»åƒç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹è‰²ã®æŒ‡é‡
        
    Returns:
        str: æœ€åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    character_keys = [E_category[speaking_rate][pitch_category], O_category[speaking_rate][pitch_category],
                      C_category[speaking_rate][pitch_category], A_category[speaking_rate][pitch_category]]
    first_prompt = (
        instruction_1
        + Extraversion[character_keys[0]] + "\n"
        + Openness[character_keys[1]] + "\n"
        + Conscientiousness[character_keys[2]] + "\n"
        + Agreeableness[character_keys[3]] + "\n"
        + "his voice is " + contrast_category + " and " + pitch_height + "\n"
        + image_color_lineage + "\n"
        + example_1
    )

    return first_prompt

def generate_second_prompt(first_prompt):
    """
    æœ€åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹
    
    Args:
        first_prompt (str): æœ€åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
    Returns:
        str: è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    # GPT-4ãƒ¢ãƒ‡ãƒ«ã‚’æƒ³å®šã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¡Œã†
    response = openai.ChatCompletion.create(
        model="gpt-4",  # ã“ã®ãƒ¢ãƒ‡ãƒ«åã¯å°†æ¥çš„ã«åˆ©ç”¨å¯èƒ½ã«ãªã‚‹ã“ã¨ã‚’æƒ³å®š
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": first_prompt}
        ]
    )
    
    # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¦æ•´å½¢
    generated_text = response.choices[0].message['content'].strip()
    
    # è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å½¢æˆ
    second_prompt = instruction_2 + "\n" + generated_text
    
    return second_prompt

def generate_second_prompt_with_expression(second_prompt, expression):
    """
    è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¡¨æƒ…ã‚’è¿½åŠ ã™ã‚‹
    
    Args:
        second_prompt (str): è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        expression (str): è¡¨æƒ…
        
    Returns:
        str: è¡¨æƒ…ã‚’å«ã‚€è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    second_prompt_with_expression = f"{second_prompt} \nExpression: {expression} \n prompt:"
    return second_prompt_with_expression

def generate_final_prompt(second_prompt):
    """
    è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰æœ€çµ‚çš„ãªç”»åƒç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹
    
    Args:
        second_prompt (str): è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
    Returns:
        str: æœ€çµ‚çš„ãªç”»åƒç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    # GPT-4ãƒ¢ãƒ‡ãƒ«ã‚’æƒ³å®šã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¡Œã†
    response = openai.ChatCompletion.create(
        model="gpt-4",  # ã“ã®ãƒ¢ãƒ‡ãƒ«åã¯å°†æ¥çš„ã«åˆ©ç”¨å¯èƒ½ã«ãªã‚‹ã“ã¨ã‚’æƒ³å®š
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": second_prompt}
        ]
    )
    
    # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¦æ•´å½¢
    final_prompt = response.choices[0].message.content

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å±•é–‹ã—ã¦è¡¨ç¤º
    expander = st.expander('prompt')
    expander.write(final_prompt)
    
    return final_prompt

def generate_image(final_prompt):
    """
    æœ€çµ‚çš„ãªç”»åƒç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰OpenAI DALL-Eã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’ç”Ÿæˆã—ã€ãã®ç”»åƒã®URLã‚’å–å¾—ã™ã‚‹
    
    Args:
        final_prompt (str): æœ€çµ‚çš„ãªç”»åƒç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
    Returns:
        str: ç”Ÿæˆã•ã‚ŒãŸç”»åƒã®URL
    """
    response = openai.Image.create(
        model="dall-e-3",
        prompt=final_prompt,
        n=1,
        quality="hd",
        size="1024x1024"
    )
    image_url = response.data[0].url  # ç”»åƒã®URLã‚’å–å¾—
    return image_url

def pipe_generate(second_prompt, expression, file_name):
    """
    è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨è¡¨æƒ…ã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã—ã€è¡¨ç¤ºãŠã‚ˆã³ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
    
    Args:
        second_prompt (str): è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        expression (str): è¡¨æƒ…
        file_name (str): éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    second_prompt_with_expression = generate_second_prompt_with_expression(second_prompt, expression)
    final_prompt = generate_final_prompt(second_prompt_with_expression)
    image_URL = generate_image(final_prompt)
    key = expression
    
    # ç”Ÿæˆã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º
    st.image(image_URL, caption=expression)
    
    # ç”»åƒã®å–å¾—ã€è¡¨ç¤ºã€ãŠã‚ˆã³ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    get_image_data(image_URL, key, file_name)

def get_image_data(image_url, key, file_name):
    """
    ç”»åƒURLã‹ã‚‰ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€è¡¨ç¤ºã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’æä¾›ã™ã‚‹
    
    Args:
        image_url (str): ç”»åƒã®URL
        key (str): ã‚­ãƒ¼å€¤
        file_name (str): éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    response = requests.get(image_url)
    image_data = response.content

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
    st.download_button(
        label="Download Image -" + str(key),
        data=image_data,
        file_name=str(key) + file_name + ".png",
        mime="image/png",
        key=key
    )

def main():
    st.title("DIALS2 - ã‚­ãƒ£ã‚¹ãƒˆã‚¤ãƒ©ã‚¹ãƒˆç”Ÿæˆ")
    uploaded_file = st.file_uploader("ğŸ‘‡éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["mp3"])
    
    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix=uploaded_file.name[-4:]) as temp_file:
            temp_file.write(uploaded_file.getvalue())
            temp_file_path = temp_file.name
            file_name = uploaded_file.name
            
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´ã‚’è¨ˆç®—
            features = compute_features(temp_file_path)
            
            # ç‰¹å¾´ã‹ã‚‰æ€§æ ¼ç‰¹æ€§ã‚’æ¨å®š
            pitch_category, contrast_category, pitch, contrast, pitch_height, image_color_lineage = categorize_audio_by_average(temp_file_path)
            characters_per_second, speaking_rate = speaking_rate_by_audio(temp_file_path)
            
            # æ€§æ ¼ç‰¹æ€§ã‹ã‚‰åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            first_prompt = generate_first_prompt(pitch_category, contrast_category, speaking_rate, pitch_height, image_color_lineage)
            
            # åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            second_prompt = generate_second_prompt(first_prompt)
            
            # è©³ç´°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆ
            image_url = generate_image(second_prompt)
            
            # ç”Ÿæˆã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º
            st.image(image_url, caption="nomal Image")
            
            # ç”»åƒã®å–å¾—ã€è¡¨ç¤ºã€ãŠã‚ˆã³ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            key = "nomal_image"
            get_image_data(image_url, key, file_name)
            
            # ç”»åƒç”Ÿæˆã¨ä¿å­˜ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
            pipe_generate(second_prompt, "serious", file_name)
            pipe_generate(second_prompt, "grinning", file_name)
            pipe_generate(second_prompt, "winking", file_name)
            pipe_generate(second_prompt, "laughing", file_name)
            pipe_generate(second_prompt, "smiling", file_name)

if __name__ == "__main__":
    password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")
    if password == st.secrets["password"]:
        main() 
    else:
        st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")